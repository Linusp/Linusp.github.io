---
title: "论文笔记：LSTM: A Search Space Odyssey"
author: Linusp
layout: post
categories: 论文笔记
---
<div id="table-of-contents">
<h2>&#30446;&#24405;</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org514ac0e">作者</a></li>
<li><a href="#org4d56869">观点</a></li>
<li><a href="#org3bd67a7">数据集</a></li>
<li><a href="#orgf2acd86">模型/实验/结论</a></li>
</ul>
</div>
</div>

<hr />

<div id="outline-container-org514ac0e" class="outline-2">
<h2 id="org514ac0e">作者</h2>
<div class="outline-text-2" id="text-org514ac0e">
<ul class="org-ul">
<li>Klaus Greff</li>
<li>Rupesh Kumar Srivastava</li>
<li>Jan Koutnik</li>
<li>Bas R. Steunebrink</li>
<li>Jurgen Schmidhuber</li>
</ul>
</div>
</div>

<div id="outline-container-org4d56869" class="outline-2">
<h2 id="org4d56869">观点</h2>
<div class="outline-text-2" id="text-org4d56869">
<ul class="org-ul">
<li>LSTM 结构的核心思想是其能维护历史状态的记忆单元，以及能调节信息出入的非线性门控单元(gating unit)</li>
<li>自 LSTM 提出后，陆续有多种对 LSTM 结构的改进工作，并广泛应用到了许多规模、性质迥异的问题上，但却缺乏对 LSTM 及其变体中各个计算部件的系统性分析</li>
</ul>
</div>
</div>

<div id="outline-container-org3bd67a7" class="outline-2">
<h2 id="org3bd67a7">数据集</h2>
<div class="outline-text-2" id="text-org3bd67a7">
<ul class="org-ul">
<li><a href="https://catalog.ldc.upenn.edu/LDC93S1">TIMIT</a></li>
<li><a href="http://www.fki.inf.unibe.ch/databases/iam-on-line-handwriting-database">IAM 在线手写数据库</a></li>
<li><a href="http://www.jsbchorales.net/index.shtml%20http://www-etud.iro.umontreal.ca/~boulanni/JSB%20Chorales.zip">JSB Chorales 复调音乐数据集</a></li>
</ul>
</div>
</div>

<div id="outline-container-orgf2acd86" class="outline-2">
<h2 id="orgf2acd86">模型/实验/结论</h2>
<div class="outline-text-2" id="text-orgf2acd86">
<p>
模型，LSTM 即八种待对比的 LSTM 变体
</p>
<ul class="org-ul">
<li>V: vanilla LSTM，即经典的 LSTM 模型</li>
<li>NIG: 在 LSTM 基础上去除 input gate 得到的结构</li>
<li>NFG: 在 LSTM 基础上去除 forget gate 得到的结构</li>
<li>NOG: 在 LSTM 基础上去除 output gate 得到的结构</li>
<li>NIAF: 在 LSTM 基础上去除 input activation function 得到的结构</li>
<li>NOAF: 在 LSTM 基础上去除 output activation function 得到的结构</li>
<li>NP: 在 LSTM 基础上去除 peephole 得到的结构</li>
<li>CIFG: 既 GRU</li>
<li>FGR: 在 LSTM 基础上让门控单元互相之间都有连接(full gate recurrence)</li>
</ul>

<p>
实验
</p>
<ul class="org-ul">
<li>在 TIMIT 数据集和 IAM 在线手写数据库上使用双向 LSTM，在 JSB Chorales 数据集上使用 LSTM</li>
<li>在 TIMIT 数据集和 JSB Chorales 上使用交叉熵作为损失函数，在 TIMIT 数据集上则使用 CTC</li>
<li>对总共 27 个模型各进行 200 次对数尺度上的超参搜索，然后进行训练，共进行 5400 次实验</li>
<li>每个数据集上的每个变体的 200 次实验中，最好的 20 个实验结果被拿来和 vanilla LSTM 模型的结果对比</li>
</ul>

<p>
结论
</p>
<ul class="org-ul">
<li>在三个数据集上，移除 forget gate 或 output activation function 都严重损害了模型性能，forget gate 对 LSTM 来说至关重要</li>
<li>对连续实数数据上的监督学习问题，input gate、output gate 和 input activation function 的存在非常重要</li>
<li>GRU 和 移除 peephole 的变体相比 vanilla LSTM 没有显著的性能差异，但它们都在一定程度上简化了 LSTM 结构</li>
<li>full gate recurrence 结构没有改善 LSTM 的性能，相反还在 JSB Chorales 数据集上让结果变差了不少，加上它让 LSTM 更复杂了，不建议使用</li>
<li>动量项对性能和训练速度都没有提高作用</li>
<li>梯度裁剪会损害整体性能</li>
<li>在使用 SGD 进行训练时，动量项对训练没什么显著好处；但在用 BSGD 进行训练时可能会起到一定的作用</li>
<li>学习率和网络大小是 LSTM 中非常重要的两个超参</li>
</ul>
</div>
</div>
